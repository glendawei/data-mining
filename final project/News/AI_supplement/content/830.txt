The week in AI: OpenAI attracts deep-pocketed rivals in Anthropic and Musk Keeping up with an industry as fast-moving as AI is a tall order. So until an AI can do it for you, here’s a handy roundup of the last week’s stories in the world of machine learning, along with notable research and experiments we didn’t cover on their own. The biggest news of the last week (we politely withdraw our Anthropic story from consideration) was the announcement of Bedrock, Amazon’s service that provides a way to build generative AI apps via pretrained models from startups including AI21 Labs, Anthropic and Stability AI. Currently available in “limited preview,” Bedrock also offers access to Titan FMs (foundation models), a family of AI models trained in-house by Amazon. It makes perfect sense that Amazon would want to have a horse in the generative AI race. After all, the market for AI systems that create text, audio, speech and more could be worth more than $100 billion by 2030, according to Grand View Research. But Amazon has a motive beyond nabbing a slice of a growing new market. In a recent Motley Fool piece, Timothy Green presented compelling evidence that Amazon’s cloud business could be slowing. The company reported 27% year-over-year revenue growth for its cloud services in Q3 2022, but the uptick slowed to a mid-20% rate by the tail-end of the quarter. Meanwhile, operating margin for Amazon’s cloud division was down 4 percentage points year over year in the same quarter, suggesting that Amazon expanded too quickly. Amazon clearly has high hopes for Bedrock, going so far as to train the aforementioned in-house models ahead of the launch — which was likely not an insignificant investment. And lest anyone cast doubt on the company’s seriousness about generative AI, Amazon hasn’t put all of its eggs in one basket. It this week made CodeWhisperer, its system that generates code from text prompts, free for individual developers. So, will Amazon capture a meaningful piece of the generative AI space and, in the process, reinvigorate its cloud business? It’s a lot to hope for — especially considering the tech’s inherent risks. Time will tell, ultimately, as the dust settles in generative AI and competitors large and small emerge. Here are the other AI headlines of note from the past few days: Image Credits: Google / Stanford University Image Credits: Google / Stanford University https://techcrunch.com/2023/04/13/chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot/?utm_source=internal&utm_medium=WPunit Image Credits: Meta Image Credits: Meta Meta open-sourced a popular experiment that let people animate drawings of people, however crude they were. It’s one of those unexpected applications of the tech that is both delightful yet totally trivial. Still, people liked it so much that Meta is letting the code run free so anyone can build it into something. Another Meta experiment, called Segment Anything, made a surprisingly large splash. LLMs are so hot right now that it’s easy to forget about computer vision — and even then, it’s a specific part of the system that most people don’t think about. But segmentation (identifying and outlining objects) is an incredibly important piece of any robot application, and as AI continues to infiltrate “the real world” it’s more important than ever that it can… well, segment anything. Image Credits: Meta Image Credits: Meta Professor Stuart Russell has graced the TechCrunch stage before, but our half-hour conversations only scratch the surface of the field. Fortunately the man routinely gives lectures and talks and classes on the topic, which due to his long familiarity with it are very grounded and interesting, even if they have provocative names like “How not to let AI destroy the world.” You should check out this recent presentation, introduced by another TC friend, Ken Goldberg:  