A road map for artificial intelligence policy 
 The rapid development of artificial intelligence technologies around the globe has led to increasing calls for robust AI policy: laws that let innovation flourish while protecting people from privacy violations, exploitive surveillance, biased algorithms, and more. But the drafting and passing of such laws has been anything but easy. “This is a very complex problem,” Luis Videgaray PhD ’98, director of MIT’s AI Policy for the World Project, said in a lecture on Wednesday afternoon. “This is not something that will be solved in a single report. This has got to be a collective conversation, and it will take a while. It will be years in the making.” Throughout his talk, Videgaray outlined an ambitious vision of AI policy around the globe, one that is sensitive to economic and political dynamics, and grounded in material fairness and democratic deliberation.    “Trust is probably the most important problem we have,” Videgaray said. Videgaray’s talk, “From Principles to Implementation: The Challenge of AI Policy Around the World,” was part of the Starr Forum series of public discussions about topics of global concern. The Starr Forum is hosted by MIT’s Center for International Studies. Videgaray gave his remarks to a standing-room crowd of over 150 in MIT’s Building E25. Videgaray, who is also a senior lecturer at the MIT Sloan School of Management, previously served as the finance minister of Mexico from 2012 to 2016, and foreign minister of Mexico from 2017 to 2018. Videgaray has also worked extensively in investment banking. Information lag and media hype In his talk, Videgaray began by outlining several “themes” related to AI that he thinks policymakers should keep in mind. These include government uses of AI; the effects of AI on the economy, including the possibility it could help giant tech firms consolidate market power; social responsibility issues, such as privacy, fairness, and bias; and the implications of AI for democracy, at a time when bots can influence political discussion. Videgaray also noted a “geopolitics” of AI regulation — from China’s comprehensive efforts to control technology to the looser methods used in the U.S. Videgaray observed that it is difficult for AI regulators to stay current with technology. “There’s an information lag,” Videgaray said. “Things that concern computer scientists today might become the concerns of policymakers a few years in the future.” Moreover, he noted, media hype can distort perceptions of AI and its applications. Here Videgaray contrasted the recent report of MIT’s Task Force on the Future of Work, which finds uncertainty about how many jobs will be replaced with technology, with a recent television documentary presenting a picture of automated vehicles replacing all truck drivers. “Clearly the evidence is nowhere near [indicating] that all jobs in truck driving, in long-distance driving, are going to be lost,” he said. “That is not the case.” With these general issues in mind, what should policymakers do about AI now? Videgaray offered several concrete suggestions. For starters: Policymakers should no longer just outline general philosophical principles, something that has been done many times, with a general convergence of ideas occurring. “Working on principles has very, very small marginal returns,” Videgaray said. “We can go to the next phase … principles are a necessary but not sufficient condition for AI policy. Because policy is about making hard choices in uncertain conditions.” Indeed, he emphasized, more progress can be made by having many AI policy decisions be particular to specific industries. When it comes to, say, medical diagnostics, policymakers want technology “to be very accurate, but you also want it to be explainable, you want it to be fair, without bias, you want the information to be secure … there are many objectives that can conflict with each other. So, this is all about the tradeoffs.”  In many cases, he said, algorithm-based AI tools could go through a rigorous testing process, as required in some other industries: “Pre-market testing makes sense,” Videgaray said. “We do that for drugs, clinical trials, we do that for cars, why shouldn’t we do pre-market testing for algorithms?” But while Videgaray sees value in industry-specific regulations, he is not as keen on having a patchwork of varying state-level AI laws being used to regulate technology in the U.S. “Is this a problem for Facebook, for Google? I don’t think so,” Videgaray said. “They have enough resources to navigate through this complexity. But what about startups? What about students from MIT or Cornell or Stanford that are trying to start something, and would have to go through, at the extreme, 55 [pieces of] legislation?” A collaborative conversation At the event, Videgaray was introduced by Kenneth Oye, a professor of political science at MIT who studies technological regulation, and who asked Videgaray questions after the lecture. Among other things, Oye suggested U.S. states could serve as a useful laboratory for regulatory innovation. “In an area characterized by significant uncertainty, complexity, and controversy, there can be benefits to experimentation, having different models being pursued in different areas to see which works best or worse,” Oye suggested. Videgaray did not necessarily disagree, but emphasized the value of an eventual convergence in regulation. The U.S. banking industry, he noted, also followed this trajectory, until “eventually the regulation we have for finance [became] federal,” rather than determined by states. Prior to his remarks, Videgaray acknowledged some audience members, including his PhD thesis adviser at MIT, James Poterba, the Mitsui Professor of Economics, whom Videgaray called “one of the best teachers, not only in economics but about a lot of things in life.” Mexico’s Consul General in Boston, Alberto Fierro, also attended the event. Ultimately, Videgaray emphasized to the audience, the future of AI policy will be collaborative. “You cannot just go to a computer lab and say, ‘Okay, get me some AI policy,’” he stressed. “This has got to be a collective conversation.” 